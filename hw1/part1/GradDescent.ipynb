{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "\n",
    "    def train(self,X,y,learning_rate=1e-3, num_iters=100,verbose=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Train a linear model using gradient descent.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: 1-dimensional array of length N of training data. \n",
    "        - y: 1-dimensional array of length N with values in the reals.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "    \n",
    "        J_history = []\n",
    "\n",
    "        # Initialize self.theta\n",
    "        if self.theta is None:\n",
    "            # lazily initialize theta \n",
    "            self.theta = np.zeros((X.shape[1],))\n",
    "\n",
    "        # Run gradient descent to find theta\n",
    "        for i in range(num_iters):\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X, y)\n",
    "\n",
    "            # add loss to J_history\n",
    "            J_history.append(loss)\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Update the parameters using the gradient and the learning rate.       #\n",
    "            #    One line of code expected                                          #\n",
    "            #########################################################################\n",
    "            self.theta = self.theta - learning_rate * grad            \n",
    "            #########################################################################\n",
    "            #                       END OF YOUR CODE                                #\n",
    "            #########################################################################\n",
    "\n",
    "            # print loss every 1000 iterations\n",
    "            if verbose and i % 1000 == 0:\n",
    "                print ('iteration %d / %d: loss %f' % (i, num_iters, loss))\n",
    "\n",
    "        return J_history\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "        Subclasses will override this.\n",
    "\n",
    "        Inputs:\n",
    "        - X: vector of length N with real values\n",
    "        - y: 1-dimensional array of length N with real values.\n",
    "\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.theta; an array of the same shape as theta\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: vector of length N of training data. \n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted output for the data in X. y_pred is a 1-dimensional\n",
    "        array of length N, and each element is a real number.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted outputs in y_pred.           #\n",
    "        #    One line of code expected                                            #\n",
    "        ###########################################################################\n",
    "        y_pred = np.dot(X, self.theta)\n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return y_pred\n",
    "  \n",
    "\n",
    "class LinearReg_SquaredLoss(LinearRegressor):\n",
    "    \"A subclass of Linear Regressors that uses the squared error loss function \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Function that returns loss and gradient of loss with respect to (X, y) and\n",
    "    self.theta\n",
    "        - loss J is a single float\n",
    "        - gradient with respect to self.theta is an array of the same shape as theta\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def loss (self,X,y):\n",
    "        J = 0\n",
    "        grad = np.zeros((2,))\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Calculate J (loss) and grad (gradient) wrt to X,y, and self.theta.      #\n",
    "        #   2-4 lines of code expected                                            #\n",
    "        ###########################################################################\n",
    "        m, y_pred = X.size, predict(X)\n",
    "        diff = y_pred-y\n",
    "        J = (1/(2*m))*np.dot(diff,diff)\n",
    "        grad = (1/m)*np.dot(diff, X)\n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1,2,3,4,5])\n",
    "np.dot(X,X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
